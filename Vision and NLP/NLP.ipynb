{
 "cells": [
  {
   "cell_type": "code",
   "id": "initial_id",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T18:46:53.557019Z",
     "start_time": "2024-11-19T18:46:43.402302Z"
    }
   },
   "source": [
    "# import sounddevice as sd\n",
    "# import scipy.io.wavfile as wav\n",
    "# fs = 44100  # Sampling frequency\n",
    "# duration = 10  # Duration in seconds\n",
    "# print(\"Start talking\")\n",
    "# myrecording = sd.rec(duration * fs, samplerate=fs, channels=2, dtype='int16')\n",
    "# sd.wait()  # Wait until recording is finished\n",
    "# print(\"Finished talking\")\n",
    "#\n",
    "# # Save to file\n",
    "# wav.write(\"output.wav\", fs, myrecording)"
   ],
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Start talking\n",
      "Finished talking\n"
     ]
    }
   ],
   "execution_count": 40
  },
  {
   "metadata": {},
   "cell_type": "code",
   "outputs": [],
   "execution_count": null,
   "source": [
    "import wave\n",
    "import pyaudio\n",
    "#Records an audio and saves it to a file\n",
    "CHUNK = 1024\n",
    "FORMAT = pyaudio.paInt16\n",
    "CHANNELS = 2\n",
    "RATE = 44100\n",
    "RECORD_SECONDS = 10\n",
    "WAVE_OUTPUT_FILENAME = \"output.wav\"\n",
    "p = pyaudio.PyAudio()\n",
    "stream = p.open(format=FORMAT,\n",
    "                channels=CHANNELS,\n",
    "                rate=RATE,\n",
    "                input=True,\n",
    "                frames_per_buffer=CHUNK)\n",
    "print(\"* recording\")\n",
    "frames = []\n",
    "for i in range(0, int(RATE / CHUNK * RECORD_SECONDS)):\n",
    "    data = stream.read(CHUNK)\n",
    "    frames.append(data)\n",
    "\n",
    "print(\"* recording done\")\n",
    "stream.stop_stream()\n",
    "stream.close()\n",
    "p.terminate()\n",
    "wf = wave.open(WAVE_OUTPUT_FILENAME, 'wb')\n",
    "wf.setnchannels(CHANNELS)\n",
    "wf.setsampwidth(p.get_sample_size(FORMAT))\n",
    "wf.setframerate(RATE)\n",
    "wf.writeframes(b''.join(frames))\n",
    "wf.close()"
   ],
   "id": "91d64ff3ae69c01e"
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T19:08:41.732653Z",
     "start_time": "2024-11-22T19:08:36.449399Z"
    }
   },
   "cell_type": "code",
   "source": [
    "import torch\n",
    "from transformers import pipeline\n",
    "import torchaudio\n",
    "import torchaudio.transforms as T\n",
    "#Pre-process the audio\n",
    "waveform, sample_rate = torchaudio.load('output.wav')\n",
    "if waveform.shape[0] > 1:\n",
    "   waveform = waveform.mean(dim=0)\n",
    "resampler = T.Resample(orig_freq=sample_rate, new_freq=16000)\n",
    "waveform = resampler(waveform)\n",
    "waveform = waveform.squeeze()\n",
    "waveform_numpy = waveform.numpy()"
   ],
   "id": "ae1b2a16a150893b",
   "outputs": [],
   "execution_count": 1
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-19T19:57:56.107671Z",
     "start_time": "2024-11-19T19:57:56.104474Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# orig_freq = sample_rate\n",
    "# new_freq = 16000\n",
    "#\n",
    "# resampler = T.Resample(orig_freq=orig_freq, new_freq=new_freq)\n",
    "# waveform = resampler(waveform)"
   ],
   "id": "74d36a4871fa4c88",
   "outputs": [],
   "execution_count": 2
  },
  {
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T18:52:06.921355Z",
     "start_time": "2024-11-22T18:52:06.658079Z"
    }
   },
   "cell_type": "code",
   "source": [
    "# print(torch.cuda.device_count())\n",
    "# for i in range(torch.cuda.device_count()):\n",
    "#     print(f\"Device {i}: {torch.cuda.get_device_name(i)}\")\n",
    "#\n",
    "# print(torch.cuda.current_device())\n",
    "# print(f\"Active GPU: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "#\n",
    "# torch.cuda.set_device(1)  # Set to GPU 1 (0-based indexing)\n",
    "# print(f\"Using device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")\n",
    "#torch.cuda.set_device(1)\n",
    "#print(f\"Using device: {torch.cuda.get_device_name(torch.cuda.current_device())}\")"
   ],
   "id": "617f5c7599b136e6",
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2\n",
      "Device 0: NVIDIA GeForce RTX 4070 SUPER\n",
      "Device 1: NVIDIA GeForce GTX 1660 Ti\n",
      "0\n",
      "Active GPU: NVIDIA GeForce RTX 4070 SUPER\n",
      "Using device: NVIDIA GeForce GTX 1660 Ti\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "cell_type": "code",
   "id": "18ae08be-d793-481b-9cce-90bac05b96fe",
   "metadata": {
    "ExecuteTime": {
     "end_time": "2024-11-22T19:08:47.932515Z",
     "start_time": "2024-11-22T19:08:42.092496Z"
    }
   },
   "source": [
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "pipe = pipeline(\"automatic-speech-recognition\",model=\"openai/whisper-large-v2\",chunk_length_s=30,device=device)\n",
    "prediction = pipe(waveform_numpy, batch_size=8)\n",
    "print(prediction[\"text\"])"
   ],
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "C:\\Python\\PyProjects\\WhisperVenv\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
      "  warnings.warn(\n",
      "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
      "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
      "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results.\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      " Ok, so now I'm testing another way to record this audio and that's it. Let's find out what happens.\n"
     ]
    }
   ],
   "execution_count": 3
  },
  {
   "metadata": {},
   "cell_type": "markdown",
   "source": [
    "C:\\Python\\PyProjects\\WhisperVenv\\.venv\\Lib\\site-packages\\transformers\\models\\whisper\\generation_whisper.py:509: FutureWarning: The input name `inputs` is deprecated. Please make sure to use `input_features` instead.\n",
    "  warnings.warn(\n",
    "Due to a bug fix in https://github.com/huggingface/transformers/pull/28687 transcription using a multilingual Whisper will default to language detection followed by transcription instead of translation to English.This might be a breaking change for your use case. If you want to instead always translate your audio to English, make sure to pass `language='en'`.\n",
    "Passing a tuple of `past_key_values` is deprecated and will be removed in Transformers v4.43.0. You should pass an instance of `EncoderDecoderCache` instead, e.g. `past_key_values=EncoderDecoderCache.from_legacy_cache(past_key_values)`.\n",
    "The attention mask is not set and cannot be inferred from input because pad token is same as eos token. As a consequence, you may observe unexpected behavior. Please pass your input's `attention_mask` to obtain reliable results."
   ],
   "id": "d0ee8bfc190ba446"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
